---
title: "Lab11"
author: "Ben Porter"
output:
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loan data
```{r}
loans <- read.table("loans2.dat", header = F,
                    col.names = c("ID", "Type", "x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8"))
loans$Type[loans$Type == 1] <- "good"
loans$Type[loans$Type == 2] <- "bad"
loans$Type <- as.factor(loans$Type)
```

## Preliminary data analysis
```{r}
# Boxplot of the two groups
library(ggplot2)
library(reshape2)
qplot(Type, value, data = melt(loans, id.vars = 1:2), geom = "boxplot") +
  facet_wrap("variable", nrow = 2, scales = "free_y") +
  labs(y = NULL)

# MANOVA for testing the means of the two groups (same as two-sample Hotelling T2 test)
library(car)
fit.lm <- lm(cbind(x1, x2, x3, x4, x5, x6, x7, x8) ~ Type, data = loans)
fit.manova <- Manova(fit.lm)
summary(fit.manova)

library(DescTools)
HotellingsT2Test(cbind(x1, x2, x3, x4, x5, x6, x7, x8) ~ Type, data = loans)
```

## LDA
```{r}
# 1. LDA without cross validation
library(MASS)
loans.lda <- lda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,
                 data = loans, prior = c(.2, .8), CV = F)  # CV = F by default
predict(loans.lda)$class                                   # predicted class on the training data
table(loans$Type, predict(loans.lda)$class)
mean(loans$Type != predict(loans.lda)$class)               # miss-classification error: (10 + 1) / 68 = 16.2%

# calculate discriminant function by hand
loans.good <- loans[loans$Type == "good", ]
loans.bad <- loans[loans$Type == "bad", ]
ave.good <- colMeans(loans.good[, -c(1, 2)])
ave.bad <- colMeans(loans.bad[, -c(1, 2)])
var.good <- var(loans.good[, -c(1, 2)])
var.bad <- var(loans.bad[, -c(1, 2)])
var.pooled <- (var.good + var.bad) / 2
lda.coef <- (ave.bad - ave.good) %*% solve(var.pooled)     # (mu1 - mu2)' * (Sigma)^(-1)
lda.coef / t(loans.lda$scaling)                            # have a coefficient between this and lda function

# 2. LDA with leave-one-out cross validation
loanscv.lda <- lda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,
                   data = loans, prior = c(.2, .8), CV = T)
loanscv.lda$class                                          # predicted class on the training data
table(loans$Type, loanscv.lda$class)
mean(loans$Type != loanscv.lda$class)                      # miss-classification error: (11 + 3) / 68 = 20.6%
```

## Exercise 1

```{r}
loans.lda <- lda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,
                 data = loans, prior = c(.15, .85), CV = F) 

predict(loans.lda)$class

table(loans$Type, predict(loans.lda)$class)
mean(loans$Type != predict(loans.lda)$class)




```

Error rate is 17.6%. 11 miss-classified bad loans. 1 miss-classified good loan.

## QDA
```{r}
# 1. QDA without cross validation
loans.qda <- qda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,
                 data = loans, prior = c(.2, .8), CV = F)  # CV = F by default
table(loans$Type, predict(loans.qda)$class)
mean(loans$Type != predict(loans.qda)$class)               # miss-classification error: (3 + 5) / 68 = 11.8%

# 2. QDA with leave-one-out cross validation
loanscv.qda <- qda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,
                   data = loans, prior = c(.2, .8), CV = T)
table(loans$Type, loanscv.qda$class)
mean(loans$Type != loanscv.qda$class)                      # miss-classification error: (8 + 8) / 68 = 23.5%
```

## Exercise 2

```{r}

loans.qda <- qda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,
                 data = loans, prior = c(.15, .85), CV = F)  # CV = F by default

predict(loans.qda)$class

table(loans$Type, predict(loans.qda)$class)
mean(loans$Type != predict(loans.qda)$class)  

```

Error rate is 10.3%. 3 miss-classified bad loans. 4 miss-classified good loan.

QDA is better in terms of training error because it has a lower error rate.


## Exercise 3
```{r}
index.bad <- which(loans$Type == "bad")    # 1:34
index.good <- which(loans$Type == "good")  # 35:68
error.lda1 <- error.lda2 <- error.lda3 <- error.qda1 <- error.qda2 <- error.qda3 <- c()

set.seed(475575)
for (i in 1:50) {
  train.index.bad <- sample(index.bad, size = 23)
  train.index.good <- sample(index.good, size = 23)
  train.index <- c(train.index.bad, train.index.good)
  
  lda1 <- lda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = loans, prior = c(.2, .8), subset = train.index)
  lda2 <- lda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = loans, prior = c(.3, .7), subset = train.index)
  lda3 <- lda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = loans, prior = c(.4, .6), subset = train.index)
  
  qda1 <- qda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = loans, prior = c(.2, .8), subset = train.index)
  qda2 <- qda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = loans, prior = c(.3, .7), subset = train.index)
  qda3 <- qda(Type ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = loans, prior = c(.4, .6), subset = train.index)
  
  error.lda1[i] <- mean(predict(lda1, loans[-train.index, ])$class != loans[-train.index, ]$Type)
  error.lda2[i] <- mean(predict(lda2, loans[-train.index, ])$class != loans[-train.index, ]$Type)
  error.lda3[i] <- mean(predict(lda3, loans[-train.index, ])$class != loans[-train.index, ]$Type)
  
  error.qda1[i] <- mean(predict(qda1, loans[-train.index, ])$class != loans[-train.index, ]$Type)
  error.qda2[i] <- mean(predict(qda2, loans[-train.index, ])$class != loans[-train.index, ]$Type)
  error.qda3[i] <- mean(predict(qda3, loans[-train.index, ])$class != loans[-train.index, ]$Type)
}
colMeans(cbind(error.lda1, error.lda2, error.lda3, error.qda1, error.qda2, error.qda3))
```

LDA is better because all 3 average error rates are lower than the 3 average error rates with QDA.

LDA with prior (0.4, 0.6) is the best.
